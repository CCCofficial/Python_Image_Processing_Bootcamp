{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week3_Part5_Tracking.ipynb","provenance":[{"file_id":"1VpN3X5JAX_KRWgQOeBZMsRGxJAMPQnP8","timestamp":1653173699849}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction to Object Tracking"],"metadata":{"id":"dCj1Lw1bGUaQ"}},{"cell_type":"markdown","source":["**Instructional Video on Tracking**\n","\n","https://youtu.be/Ky8J56lbm3c \n","\n","**Simple Approach to Object Tracking**\n","\n","Each time we detect objects in a frame of video, we start fresh, forgetting about all the objects in the previous frame. But the new objects are typically the same objects in the previous frame, possibly with a small change in position due to movement. What tracking does is give each object an identification number (objID) and maintains that id as the object moves in successive frames.\n","\n","Here is the terminology I like to use. We match an **old object** in the **old frame** (previous frame) to the closest **new object** in the **new frame** (current frame). You will also notice there are print statements in the code that are commented out. I left them in there so you can see how I debug my code, by printing out signficant variables while the programming is running.\n","\n","**Simple Object Tracker Implimentation**\n","\n","We will implement a very simple tracking algorithm, called the centroid tracker, because it tracks the center of each object. The assumption is a new object in the new frame hasn't moved much since the old frame. Therefore, a new object is the same object in the old frame with the closest proximity. To do this, for every new object in the new frame, we calculate the center of the object (xc,yc), and find the object in the old frame that is closest to xc,yc. This works as long as the objects aren't too close to each other (avoiding merging and splitting), don't move too fast, and don't go in and out of frame. Once we do all the matching, any left over old objects are called **orphans**. They either left the display or we experienced detector dropout. Any left over new objects are called **newborns**, they either just came into the frame are are produced by detector **dropins**. In the next section we will go into more detail on tracking failure modes.\n","\n","**Tracking Failure Modes**\n","\n","The simple tracker works well when there are just a few objects and they don't cross or touch each other. However, there are several conditions that will cause tracking errors. We will consider three cases; when objects move in and out of frame, when objects in frame overlap each other, and when the detector fails. I'll now describe some cases where the simple tracker will fail. In the first case, if an object moves out of frame, it will disappear from the new frame creating **orphans** (old object that doesn't have a corresponding new object). Conversly, an object can move into the new frame, it will not be in the previous frame, and will appear in the new frame (**newborns**). \n","\n","In the second case, when two objects in a previous frame, overlap in the current frame, they will be considered one object, a condition we call **merging**. Conversly, when two objects that were merged in a previous frame, separate into two distinct objects in the current frame, we call this **splitting**. \n","\n","In the third case, the detection is faulty. For the simple detection method we are using, this happens when there is variation in the object brightness from frame-to-frame and the brightness is near the quantization threshold. When the brightness dips below the threshould, the object is not detected, and disappears. We call this **dropout**. When the brightness increases above the threshold, it reappears (let's call it **drop-in**), often giving the appearance of **blinking**, alternating between appearing and disappearing.\n","\n","In conclusion, if the object count in the current frame is less than the object count in the previous frame, an object left the frame, merged with another object, or detector dropout has occured. If the object count in the current frame is greater than the object count in the previous frame, an object entered the frame, splitting, or detector drop-in has occured. \n","\n","**Going Further**\n","\n","If you would like to learn about eight object tracking implementations provided in the OpenCV library, check out this link https://pyimagesearch.com/2018/07/30/opencv-object-tracking/"],"metadata":{"id":"fDjMc0bYBGUa"}},{"cell_type":"markdown","source":["**Loading the Video**\n","\n","Mount Google drive through using drive.mount('/content/drive') Ceate a shortcut to the class Google Drive home directory to the location where the data is: Open the class Google Doc Folder https://drive.google.com/drive/folders/17dFdrIbTp8RjivAuOiLNyb8pTqZ8QqgL (\"SCIP_IMAGE_PYTHON_2022\"), Right click on \"SCIP_DATA\",click \"Add shortcut to drive\".\n","\n","Run the code below to mount the class code data. You should be able to find image and video files in the folder (on the left) under drive/MyDrive/SCIP_DATA."],"metadata":{"id":"wedq7U5YzTNR"}},{"cell_type":"code","source":["#import glob\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","from IPython import display\n","from time import sleep\n","import cv2\n","drive.mount('/content/drive')"],"metadata":{"id":"423S5-2c1ZjJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Review: Detect Objects in Video"],"metadata":{"id":"08VOVTIaJf93"}},{"cell_type":"markdown","source":["Run the detection code from last week that does the following image processing steps: reads a video frame-by-frame, resizes it and converts it to grayscale, binary quantizes each grayscale frame using a fixed threshold, uses \"cv2.findCountours\" to detect the objects, selects the objects (plankton) we want by size, and draws a yellow rectangle the selected objects. Notice I added a few lines of code to calculate the center of the object. This will be useful when we try and track the objects."],"metadata":{"id":"iBhe5s9nEIZw"}},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","\n","vid='/content/drive/MyDrive/SCIP_DATA/Video/threeStentor.mp4'\n","framesToDisplay=300\n","thresh=40 # used to determine if a pixel is assigned 0 or 255\n","frameNumber=0\n","minArea=300; maxArea=2000; # in pixels\n","CROP_SIZE=4 # number of pixels to remove on each side of image\n","thick=3   # thickness of rectangle lines around detected objects\n","cap = cv2.VideoCapture(vid)\n","xRez=640; yRez=480;\n","\n","while(cap.isOpened() and frameNumber<framesToDisplay):\n","    # get image\n","    ret, frameIM = cap.read()\n","    if not ret: # check to make sure there was a frame to read\n","      print('Done with video')\n","      break\n","    frameIM = cv2.resize(frameIM, (xRez, yRez))\n","    grayIM = cv2.cvtColor(frameIM, cv2.COLOR_BGR2GRAY)    # convert color to grayscale image\n","    grayIM=grayIM[CROP_SIZE:yRez-CROP_SIZE,CROP_SIZE:xRez-CROP_SIZE]\n","    ret,binaryIM = cv2.threshold(grayIM,thresh,255,cv2.THRESH_BINARY) # threshold image to make pixels 0 or 255\n","    \n","    # detect objects in binaryIM\n","    contourList, hierarchy = cv2.findContours(binaryIM, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # all countour points, uses more memory\n","    objectCount=0 # counts number of objects detected\n","    for objContour in contourList:\n","      area = cv2.contourArea(objContour)\n","      #print(len(contourList),area)\n","      if area>minArea and area<maxArea:\n","        #print('frame',frameCount,'object',objectCount,'area',area)\n","        PO = cv2.boundingRect(objContour)\n","        x0=PO[0]; y0=PO[1]; w=PO[2]; h=PO[3]\n","        \n","        ############## NEW CODE: calculate center of object ###############\n","        xc=int(x0+w/2+0.5)\n","        yc=int(y0+h/2+0.5)\n","        ###################################################################\n","        \n","        cv2.rectangle(grayIM, (x0,y0), (x0+w,y0+h),255, thick) # place rectangle around each object, BGR\n","        objectCount+=1\n","    cv2_imshow(grayIM)\n","    sleep(0.1)\n","    display.display(plt.gcf())\n","    display.clear_output(wait=True)\n","    frameNumber+=1\n","cap.release()"],"metadata":{"id":"QTsH8qBpEB0B","executionInfo":{"status":"ok","timestamp":1658531883804,"user_tz":420,"elapsed":159,"user":{"displayName":"Anthony Bravo","userId":"12496842674527385209"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Saving Object Locations using Pickle"],"metadata":{"id":"0H51YjH2AvQk"}},{"cell_type":"markdown","source":["Now that we have the centers (xc,yc) of all the objects we want to track, we will save them in a list that we will later use to perform the tracking, without having to re-run the detect program. We could do it all in one program, but it would be rather large. To speed up processing, we are not going to display the detection results, since we know it works."],"metadata":{"id":"w7_HMTypBCg5"}},{"cell_type":"code","source":["import pickle\n","from google.colab import files\n","\n","vid='/content/drive/MyDrive/SCIP_DATA/Video/threeStentor.mp4'\n","framesToDisplay=300\n","thresh=40 # used to determine if a pixel is assigned 0 or 255\n","frameNumber=0\n","minArea=300; maxArea=2000; # in pixels\n","CROP_SIZE=4 # number of pixels to remove on each side of image\n","thick=3   # thickness of rectangle lines around detected objects\n","cap = cv2.VideoCapture(vid)\n","xRez=640; yRez=480;\n","\n","############## NEW CODE: Defining a list to store object location ###############\n","objList=[] # we will need this list to store new object location\n","###################################################################\n","\n","print('Creating objList') # I like to give a status of the program when it's running so the user know what it's doing\n","while(cap.isOpened() and frameNumber<framesToDisplay):\n","    # get image\n","    ret, frameIM = cap.read()\n","    if not ret: # check to make sure there was a frame to read\n","      print('Done reading entire video')\n","      break\n","    frameIM = cv2.resize(frameIM, (xRez, yRez))\n","    grayIM = cv2.cvtColor(frameIM, cv2.COLOR_BGR2GRAY)    # convert color to grayscale image\n","    grayIM=grayIM[CROP_SIZE:yRez-CROP_SIZE,CROP_SIZE:xRez-CROP_SIZE]\n","    ret,binaryIM = cv2.threshold(grayIM,thresh,255,cv2.THRESH_BINARY) # threshold image to make pixels 0 or 255\n","    \n","    # detect objects in binaryIM\n","    contourList, hierarchy = cv2.findContours(binaryIM, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # all countour points, uses more memory\n","    objectCount=0 # counts number of objects detected\n","    for objContour in contourList:\n","      area = cv2.contourArea(objContour)\n","      #print(len(contourList),area)\n","      if area>minArea and area<maxArea:\n","        #print('frame',frameCount,'object',objectCount,'area',area)   # used for debugging\n","        PO = cv2.boundingRect(objContour)\n","        x0=PO[0]; y0=PO[1]; w=PO[2]; h=PO[3]\n","        \n","        ############## NEW CODE: calculate center of object ###############\n","        xc=int(x0+w/2+0.5)\n","        yc=int(y0+h/2+0.5)\n","        ASSIGNED_FLAG=0\n","        objID=-1          # -1 will indicate that no id has been assigned to the object (valid id's are positive numbers, starting with 0)\n","        v=[frameNumber,xc,yc,objID,ASSIGNED_FLAG] # create a list of object location and id. You'll see!\n","        objList.append(v)  # append the object location and id to the newObjList (we just made a list of lists!)   \n","        \n","        ###################################################################\n","        \n","        cv2.rectangle(grayIM, (x0,y0), (x0+w,y0+h),255, thick) # place rectangle around each object, BGR\n","        objectCount+=1\n","\n","    # no need to display objects, just takes up time and we know it works by now\n","    #plt.imshow(grayIM)\n","    #plt.title('frame ' + str(frameNumber))\n","    #sleep(0.3)\n","    #display.display(plt.gcf())\n","    #display.clear_output(wait=True)\n","    frameNumber+=1\n","cap.release()\n","\n","# save the objList using pickle \n","pick_insert = open('drive/My Drive/objList.pickle','wb')\n","pickle.dump(objList, pick_insert)\n","pick_insert.close()\n","print('Saved objList.pickle')\n"],"metadata":{"id":"BjHvD4kgT6x8","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1658551187445,"user_tz":420,"elapsed":197,"user":{"displayName":"Anthony Bravo","userId":"12496842674527385209"}},"outputId":"21655ccf-5424-42d0-9c39-b8116db96e18"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7e5a76ab206e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mCROP_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;31m# number of pixels to remove on each side of image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mthick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m   \u001b[0;31m# thickness of rectangle lines around detected objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mxRez\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0myRez\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"]}]},{"cell_type":"markdown","source":["Now let's play back the objList to make sure it's good."],"metadata":{"id":"skOIgRTejnZp"}},{"cell_type":"code","source":["import numpy as np\n","\n","lastFrame=objList[-1][0]\n","print('lastFrame',lastFrame)\n","radius=10; thick=0;\n","color=(255,255,255)\n","im=np.zeros((yRez,xRez,3),dtype='uint8') # create a black image\n","lastFrame=0\n","for i in range (len(objList)):\n","    frame,xc,yc,objID,ASSIGNED_FLAG=objList[i] \n","    cv2.circle(im, (xc,yc), radius,color[0], thick) # place rectangle around each object, BGR\n","    if frame!=lastFrame:\n","        lastFrame=frame\n","        cv2_imshow(im)\n","        #sleep(0.3) # no need to make sure every frame is visible since we keep writing circles on the original image (like a multiple exposure in photography), leaving a trail of object locations\n","        display.display(plt.gcf())\n","        display.clear_output(wait=True)"],"metadata":{"id":"YPtUGiFijxAq","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1658551133189,"user_tz":420,"elapsed":262,"user":{"displayName":"Anthony Bravo","userId":"12496842674527385209"}},"outputId":"3600212c-2f76-48b2-aee8-315c39f6a34d"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-06ac929d6965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlastFrame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lastFrame'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlastFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mthick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'objList' is not defined"]}]},{"cell_type":"markdown","source":["# Tracking Code"],"metadata":{"id":"xsDxdPSvBAJt"}},{"cell_type":"markdown","source":["Now we are ready to do tracking using the objFile.\n","Here is an outline of what the code will be doing...\n","1. Find out where each frame starts in the objList. \n","2. Initialize the objects in the first frame by assigning them ID's, starting with 0.\n","3. Make a nested loop (a loop in a loop). For each of the old objects (outer loop) find the closest new object (inner loop), and assign the old object ID to the new object. Set the ASSIGNED_FLAG for the new object so it isn't matched multiple times to old objects.\n","4. Give any newborns (new objects that were not assigned to old objects) a new unique ID, and keep track of what new id to give with the variable newID.\n","5. Show the tracking results by displaying each frame with bounding boxes around each object, color coded by the object ID. If there is perfect tracking, the bounding box of an object will stay constant as the object moves. If the color changes, try and predict what failure mode is occuring.\n"],"metadata":{"id":"aCMWZaB_lIPK"}},{"cell_type":"code","source":["import math # we are going to need square root for calculating distance\n","\n","radius=10; thick=0;\n","# To see if the the tracker is working, let's color code the bounding box of each object by it's id.\n","# If tracking is working perfectly, an object will retain it's color. \n","# Seven colors are defined in a list. Can you decode what color they would produce? Hint: BGR\n","color=[(0,0,255),(0,255,0),(0,255,255),(255,0,0),(255,0,255,),(255,255,0),(255,255,0)] \n","\n","# The objList is a list of lists. Each list contains these variables in this order; frameNumber,xc,yc,objID,ASSIGNED_FLAG\n","# Instead of retreiving their value (indexing) with numbers 0 to 4 (remembers computers count starting with 0), I like to use labels, to give the code better readablity\n","frameIndex=0\n","idIndex=3\n","assignIndex=4\n","\n","# create a black image for us to draw on\n","im=np.zeros((yRez,xRez,3),dtype='uint8')\n","\n","# make a list of where the frame boundaries are in objList\n","startFrame=[]\n","lastFrame=0\n","for i in range (len(objList)):\n","    frame,xc,yc,objID,ASSIGNED_FLAG=objList[i] \n","    cv2.circle(im, (xc,yc), radius,color[0], thick) # place rectangle around each object, BGR\n","    if frame!=lastFrame:\n","        startFrame.append(i)\n","        lastFrame=frame\n","\n","# give objects in the first frame initial id's so they can be propagated to successive frames\n","startingID=0\n","for i in range(startFrame[0],startFrame[1]):\n","    objList[i][idIndex]=startingID\n","    startingID+=1\n","newID=startingID          # if a new id is needed for a new object, start with this since it's the next one that hasn't been used (every object must have a unique id!)\n","\n","# now lets match each object in the old (last) frame with objects in the current (new) frame\n","orphanCount=0                         # counts how many old objects didn't find matches to new objects (ran out of new objects)\n","for i in range(1,len(startFrame)-1):  # we start with frame 1 (new frame, new objects) since we will be looking back at the previous old frame (frame 0) to match old objects with new objects\n","    oldStart=startFrame[i-1]          # beginning of old frame\n","    oldStop=startFrame[i]             # end of old frame\n","    newStart=startFrame[i]            # beginning of new frame\n","    newStop=startFrame[i+1]           # end of new frame\n","    #print(oldStart,oldStop,newStart,newStop)\n","    \n","    BIG_NUMBER=999999               # you'll see how we use this very soon to find the minimum distance\n","    for iOld in range(oldStart,oldStop):  # process every old object trying to match it to the closest new object\n","        frame,xcOld,ycOld,idOld,ASSIGNED_FLAG=objList[iOld] # let's get the location and id of the old object. \n","        if ASSIGNED_FLAG==0:        # only process old objects that have not been assigned (matched) to the closest new object\n","            bestDistance=BIG_NUMBER # make this a really big number, so when we do our initial compare, it's guarantee to be smaller than this number\n","            bestMatchIndex=0        # we'll update this when we find a close object in the previous frame\n","            for iNew in range(newStart,newStop):      # check distance of every new object to the old object we are processing\n","                frame,xcNew,ycNew,_,_=objList[iNew]   # the dashes ('_') are a way to receive variables that we don't care about. It stores them in '_' which we will ignore. Sometimes I use \"dummy\", but most people seem to use '_'\n","                dx=xcNew-xcOld\n","                dy=ycNew-ycOld\n","                distance=math.sqrt(dx*dx+dy*dy)\n","                if distance<bestDistance:             # if old object is closer than the closest old object we've seen so far, save it's information\n","                  bestDistance=distance               # keep the value and see if any other object is closer\n","                  bestMatchIndex=iNew                 # keep track of the best old object in case it turns out to be the closest to the new object\n","                  #print(iNew,distance)\n","            if bestDistance!=BIG_NUMBER:              # if bestDistance has been updated, that means a match has been found \n","              objList[bestMatchIndex][idIndex]=idOld  # the new object get's the id of the old object. The third element of the list (2 because we count from zero!) is the ID\n","              objList[idOld][assignIndex]=1           # ASSIGNED_FLAG=1 indicating old object has been assigned a new oject, so it can't be matched to any other new object (one per customer!)\n","            else:   \n","              orphanCount+=1                          # after testing all the new objects, bestDistance was never updated (still has the initial value of BIG_NUMBER), so we know there was no new object to match to this old object :( \n","\n","    # now that all old objects have been matched to new objects (except for old orphans), let's see if there are any new objects that haven't been assigned id's (new born objects)\n","    # we can tell because their id is their value when we created the objFile, which is -1\n","    # if they are \"new born\", give them an id using the newID variable\n","    for iNew in range(newStart,newStop):\n","        if objList[iNew][idIndex]==-1:      # id==-1 means the new object was not matched to an old object, so give it a newID\n","            objList[iNew][idIndex]=newID\n","            newID+=1                        # make sure we advance newID so the next \"new born\" gets a unique id\n","            print('new born id', objList[iNew][idIndex])\n","\n","# now show that it works, by color coding the id of the objects\n","im=np.zeros((yRez,xRez,3),dtype='uint8')        # notice we make a color image by having three dimensions; x,y and 3 color channels (BGR)\n","lastFrame=0     # last frame is the starting frame, to start the loop (\"bootstrapping\")\n","for i in range (len(objList)):\n","    frame,xc,yc,objID,ASSIGNED_FLAG=objList[i] \n","    #print('frame',frame,'objID',objID)\n","    cv2.circle(im, (xc,yc), radius,color[objID%len(color)], thick) # place rectangle around each object, BGR\n","    if frame!=lastFrame:\n","        lastFrame=frame\n","        cv2_imshow(im)\n","        sleep(0.3)\n","        display.display(plt.gcf())\n","        display.clear_output(wait=True)\n"],"metadata":{"id":"3h2ol1eHlMcR","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1658531773028,"user_tz":420,"elapsed":266,"user":{"displayName":"Anthony Bravo","userId":"12496842674527385209"}},"outputId":"08f54d52-fe82-4a96-a55c-142fd538229b"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cb54d0682b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# create a black image for us to draw on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myRez\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxRez\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# make a list of where the frame boundaries are in objList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]}]}