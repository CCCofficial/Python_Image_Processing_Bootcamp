{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week6_FoundationsofClustering.ipynb","provenance":[{"file_id":"1SfARcv9uTpBaI0pmBOdo8l49QpwQUsyv","timestamp":1656271773038}],"collapsed_sections":[],"authorship_tag":"ABX9TyN6yvnvbaTTdhCAB6LWVQag"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CLUSTERING\n","\n","We will now discuss Clustering.  Luckily, this the foundation of machine learning and the internet is full of great tools and resources.  Below is listed some tools we like.  \n","\n","The idea of clustering is how can you convert mathematical features of images/objects (or in general any numerica data) it a form that a computer can use to  classify the object.  \n","\n","We already started the process by extracting features form the objects in an image (Weeks 1-5).  The feature extractions puts this into a quantifyable number that a computer use to sort objects by these parameters.  By \"clustering\" these values the computer can say any object that falls within the parameters (features) of a type of object is that type of object. \n","\n","Videos:\n","\n","https://www.youtube.com/watch?v=EItlUEPCIzM\n","\n","https://www.youtube.com/watch?v=H_L7V_BH9pc\n","\n","Websites:\n","\n","https://realpython.com/k-means-clustering-python/\n","\n","https://scikit-learn.org/stable/modules/clustering.html\n","\n","https://machinelearningmastery.com/clustering-algorithms-with-python/\n","\n","https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n","\n","Google Colab:\n","\n","https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb\n","\n","https://colab.research.google.com/github/SANTOSHMAHER/Machine-Learning-Algorithams/blob/master/K_Means_algorithm_using_Python_from_scratch_.ipynb\n","\n","Need some practice with NumPy:\n","\n","https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numpy_ultraquick_tutorial.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=mlcc-prework&hl=en\n","\n","Need some practice with Pandas:\n","\n","https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=mlcc-prework&hl=en\n","\n","Here is a complete crash course on machine learning (via Google Developers):\n","\n","https://developers.google.com/machine-learning/crash-course\n","\n","\n"],"metadata":{"id":"cuxULZITL7r_"}},{"cell_type":"markdown","source":["Please watch the videos and review the materials above.  In this section, we will focus on one type of clustering strategy (described above) called k-means, but there are many ways to cluster your data.  \n","\n","Below we will:\n","1) Walk thru k-means using this site as a template:\n","(https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n","\n","2) Walk thru 3D k-means using this site as template:\n","\n","https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html#sphx-glr-auto-examples-cluster-plot-cluster-iris-py\n","\n","3) Apply clustering the shape picture we covered in Week5\n"],"metadata":{"id":"h7eM_kZvffxe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_J71aMsJ-JM"},"outputs":[],"source":["from google.colab import drive\n","import matplotlib.pyplot as plt\n","from IPython import display\n","import cv2\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","import argparse\n","import seaborn as sns; sns.set()  # for plot styling\n","import imutils  #this is open source image tools (utilities) for image processing, https://anaconda.org/conda-forge/imutils, made by the creator of PyimageSearch.com\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from sklearn.datasets import make_blobs  #this feature makes data sets which specified clusters (centers) and number(n_samples) and noise (cluster_std)\n","X, y_true = make_blobs(n_samples=300, centers=4,\n","                       cluster_std=0.60, random_state=0)\n","plt.scatter(X[:, 0], X[:, 1], s=50);  #this plots the data, what is s?\n","\n","#play around with the make_blobs method return to original values (n_samples=300, centers=4,cluster_std=0.60, random_state=0)\n","\n","#the x and y axis here are just arbitrary numbers, in our case, they will represent some numeric value we extracted from the pixels that represents our image (features)\n"],"metadata":{"id":"Db59smMY3ODZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import KMeans  #this is the K-means clustering methods\n","kmeans = KMeans(n_clusters=4) #this finds the best fit to the number of clusters of size n_clusters, this step defines n_clusters and defines of object of class KMeans called kmeans \n","kmeans.fit(X) #this calculates the centroids for n_clusters.  X is our array of data, not x-axix\n","y_kmeans = kmeans.predict(X) #Predict the closest cluster each sample in X belongs to.\n","\n","#look at the variable explorer to the right, {x}, what is returned in y_kmeans?  Each number in the array corresponds to the index of X and the cluster that it was assigned.\n"],"metadata":{"id":"meKxwOxo53J7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')  #plots data X\n","\n","centers = kmeans.cluster_centers_  #this gives the coordinates of the n_clusters defined in the object kmeans of class KMeans\n","plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5);  #plots the center of the clusters"],"metadata":{"id":"hXXAJNhhAryi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","import matplotlib.pyplot as plt\n","from ipywidgets import interact\n","from sklearn.metrics import pairwise_distances_argmin\n","\n","#this is a more advanced coding to show how the program works.  Here you can compare clusters and steps in the algorithm process to show how it along the way\n","\n","def plot_kmeans_interactive(min_clusters=1, max_clusters=6):    \n","    X, y = make_blobs(n_samples=300, centers=4,\n","                      random_state=0, cluster_std=0.60)\n","        \n","    def plot_points(X, labels, n_clusters):\n","        plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis',\n","                    vmin=0, vmax=n_clusters - 1);\n","            \n","    def plot_centers(centers):\n","        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n","                    c=np.arange(centers.shape[0]),\n","                    s=200, cmap='viridis')\n","        plt.scatter(centers[:, 0], centers[:, 1], marker='o',\n","                    c='red', s=50)\n","            \n","\n","    def _kmeans_step(frame=0, n_clusters=4):\n","        rng = np.random.RandomState(2)\n","        labels = np.zeros(X.shape[0])\n","        centers = rng.randn(n_clusters, 2)\n","\n","        nsteps = frame // 3\n","\n","        for i in range(nsteps + 1):\n","            old_centers = centers\n","            if i < nsteps or frame % 3 > 0:\n","                labels = pairwise_distances_argmin(X, centers)\n","\n","            if i < nsteps or frame % 3 > 1:\n","                centers = np.array([X[labels == j].mean(0)\n","                                    for j in range(n_clusters)])\n","                nans = np.isnan(centers)\n","                centers[nans] = old_centers[nans]\n","\n","        # plot the data and cluster centers\n","        plot_points(X, labels, n_clusters)\n","        plot_centers(old_centers)\n","\n","        # plot new centers if third frame\n","        if frame % 3 == 2:\n","            for i in range(n_clusters):\n","                plt.annotate('', centers[i], old_centers[i], \n","                             arrowprops=dict(arrowstyle='->', linewidth=1))\n","            plot_centers(centers)\n","\n","        plt.xlim(-4, 4)\n","        plt.ylim(-2, 10)\n","\n","        if frame % 3 == 1:\n","            plt.text(3.8, 9.5, \"1. Reassign points to nearest centroid\",\n","                     ha='right', va='top', size=14)\n","        elif frame % 3 == 2:\n","            plt.text(3.8, 9.5, \"2. Update centroids to cluster means\",\n","                     ha='right', va='top', size=14)\n","    \n","    return interact(_kmeans_step, frame=list(range(0,50,10)),\n","                    n_clusters=list(range(min_clusters, max_clusters+1)))\n","\n","plot_kmeans_interactive();\n","\n","#here the frame # represents the numuber of inter"],"metadata":{"id":"tQc8x6S1O3p-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kmeans_kwargs = {\n","    \"init\": \"random\",\n","    \"n_init\": 10,\n","    \"max_iter\": 300,\n","    \"random_state\": 42,\n","}\n","\n","# A list holds the SSE values for each k\n","sse = []\n","for k in range(1, 11):\n","    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n","    kmeans.fit(X)\n","    sse.append(kmeans.inertia_)\n","plt.style.use(\"fivethirtyeight\")\n","plt.plot(range(1, 11), sse, '-o')\n","plt.xticks(range(1, 11))\n","plt.xlabel(\"Number of Clusters\")\n","plt.ylabel(\"SSE\")\n","plt.show()\n","\n","#the plot below shows the \"elbow method\"  The break in the curve (the elbow) at 4 shows the max number of clusters.  More quantitatively, this show the point where added clusters does not improve\n","#the residiuals (SSEs numbers)\n"],"metadata":{"id":"1aHMqqchS90_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The above example is just one way to determine how many clusters to use.  There are others and ultimetly how one determines depends on nature of the data\n","Often clustering is done in 2D scatter plots to help visualize, but it may be done in mulitiple dimensions (2 or more).  \n","\n","Below we use an example of clustering in 3D from the sci-learn website:\n","\n","https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html#sphx-glr-auto-examples-cluster-plot-cluster-iris-py\n","\n","It represents real data from the IRIS data set. It has three parameters.  It shows how cluster number and poor initial guesses (more important in data sets with more than 2 parameters.  In this case \"local minima\" are more likely depending on the number of variables)."],"metadata":{"id":"6tXU5aEoszCR"}},{"cell_type":"code","source":["# Code source: Gaël Varoquaux\n","# Modified for documentation by Jaques Grobler\n","# License: BSD 3 clause\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Though the following import is not directly being used, it is required\n","# for 3D projection to work with matplotlib < 3.2\n","import mpl_toolkits.mplot3d  # noqa: F401\n","\n","from sklearn.cluster import KMeans\n","from sklearn import datasets\n","\n","np.random.seed(5)\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","estimators = [\n","    (\"k_means_iris_8\", KMeans(n_clusters=8)),\n","    (\"k_means_iris_3\", KMeans(n_clusters=3)),\n","    (\"k_means_iris_bad_init\", KMeans(n_clusters=3, n_init=1, init=\"random\")),\n","]\n","\n","fignum = 1\n","titles = [\"8 clusters\", \"3 clusters\", \"3 clusters, bad initialization\"]\n","for name, est in estimators:\n","    fig = plt.figure(fignum, figsize=(8, 6))\n","    ax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\n","    ax.set_position([0, 0, 0.95, 1])\n","    est.fit(X)\n","    labels = est.labels_\n","\n","    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor=\"k\")\n","\n","    ax.w_xaxis.set_ticklabels([])\n","    ax.w_yaxis.set_ticklabels([])\n","    ax.w_zaxis.set_ticklabels([])\n","    ax.set_xlabel(\"Petal width\")\n","    ax.set_ylabel(\"Sepal length\")\n","    ax.set_zlabel(\"Petal length\")\n","    ax.set_title(titles[fignum - 1])\n","    ax.dist = 12\n","    fignum = fignum + 1\n","\n","# Plot the ground truth\n","fig = plt.figure(fignum, figsize=(8, 6))\n","ax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\n","ax.set_position([0, 0, 0.95, 1])\n","\n","for name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n","    ax.text3D(\n","        X[y == label, 3].mean(),\n","        X[y == label, 0].mean(),\n","        X[y == label, 2].mean() + 2,\n","        name,\n","        horizontalalignment=\"center\",\n","        bbox=dict(alpha=0.2, edgecolor=\"w\", facecolor=\"w\"),\n","    )\n","# Reorder the labels to have colors matching the cluster results\n","y = np.choose(y, [1, 2, 0]).astype(float)\n","ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\"k\")\n","\n","ax.w_xaxis.set_ticklabels([])\n","ax.w_yaxis.set_ticklabels([])\n","ax.w_zaxis.set_ticklabels([])\n","ax.set_xlabel(\"Petal width\")\n","ax.set_ylabel(\"Sepal length\")\n","ax.set_zlabel(\"Petal length\")\n","ax.set_title(\"Ground Truth\")\n","ax.dist = 12\n","\n","fig.show()"],"metadata":{"id":"MyuG4vQTWbU4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"VTIRrZlzwZNA"}},{"cell_type":"code","source":["Shapes = r'/content/drive/MyDrive/SCIP_DATA/Images/shapes.png' \n","colorIM = cv2.imread(Shapes) #Read the color image of difference shape. \n","cv2_imshow(colorIM)"],"metadata":{"id":"H1i7Vi76KRvI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Our image if too large for the algoritim to work well.  So we to The goal of this section is to 1) resize the image 2) perform detection\n","IM = colorIM  # our working image\n","resizedIM = imutils.resize(IM, width=300)  #using the open source imutils tool to resixze an image\n","ratio = IM.shape[0] / float(resizedIM.shape[0])  #we record the factor to use later to resize back to original size- why does this work?\n","\n","# convert the resized image to grayscale, blur it, and then perform threshold \n","grayIM = cv2.cvtColor(resizedIM, cv2.COLOR_BGR2GRAY)\n","blurredIM = cv2.medianBlur(grayIM,5)\n","ret,threshIM = cv2.threshold(blurredIM, 60, 255, cv2.THRESH_BINARY)\n","cv2_imshow(threshIM)\n","\n","# find contours in the thresholded image\n","contourList, hierarchy = cv2.findContours(threshIM.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","#contourIM = cv2.findContours(threshIM.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","#contourIM = imutils.grab_contours(contourIM)\n"],"metadata":{"id":"hgLM5AdLLdrq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this section we first compute the center of the contour, then use the above detect_shape function to determine the shape of the obeject based on the contour of that object. The goal of this  section is to highlight the challenges of clustering using an image that is clear to a human observer"],"metadata":{"id":"BCgbjqPfN22x"}},{"cell_type":"code","source":["IM_2 = IM.copy() #make a copy of the object\n","area = []\n","sides = []\n","aspect_ratio = []\n","for contours in contourList:\n","\tarea.append(cv2.contourArea(contours))\n","\tperi = cv2.arcLength(contours, True)\n","\tx,y,w,h = cv2.boundingRect(contours)\n","\tsides.append(min(len(cv2.approxPolyDP(contours,0.04*peri, True)),7)) #makes the maximum 7 = circle\n","\taspect_ratio.append(min(float(w)/h, h/float(w),3)) #this keeps the aspect independent of x, y position\n","area=np.array(area)\n","sides=np.array(sides)\n","aspect_ratio = np.array(aspect_ratio)\n","data_features = np.column_stack((area, sides, aspect_ratio))\n"],"metadata":{"id":"M6PRnyZ8N9eR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lets see how sides and aspect_ratio correlate on an xy plot\n","#lets see how clustering to 5 groups works\n","X2 = np.column_stack((data_features[:, 1], data_features[:, 2]))\n","kmeans2 = KMeans(n_clusters=5) \n","kmeans2.fit(X2) \n","y_kmeans2 = kmeans2.predict(X2)\n","centers2 = kmeans2.cluster_centers_  #this gives the coordinates of the n_clusters defined in the object kmeans of class KMeans\n","plt.scatter(centers2[:, 0], centers2[:, 1], c='red', s=200, alpha=0.5);\n","plt.scatter(X2[:, 0], X2[:, 1], c=y_kmeans2, s=50, cmap='viridis')\n","#this works okay but likely confuses one rectangle with a square, which one?\n","#would this work with Area versus sides?\n","\n"," "],"metadata":{"id":"muEjp0adM3AN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#would this work with Area versus sides?\n","\n","X3 = np.column_stack((data_features[:, 1], data_features[:, 0]))\n","kmeans3 = KMeans(n_clusters=5) \n","kmeans3.fit(X3) \n","y_kmeans3 = kmeans3.predict(X3)\n","centers3 = kmeans3.cluster_centers_  #this gives the coordinates of the n_clusters defined in the object kmeans of class KMeans\n","plt.scatter(centers3[:, 0], centers3[:, 1], c='red', s=200, alpha=0.5);\n","plt.scatter(X3[:, 0], X3[:, 1], c=y_kmeans3, s=50, cmap='viridis')\n","\n","#very poorly clustered.  \n"],"metadata":{"id":"2Wb1dlInpd29"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This completes this section on clustering and it is a first step on long journey of learning how we can us machines to solve problems\n"],"metadata":{"id":"q3veHDpMp8aF"}},{"cell_type":"code","source":["%reset -f"],"metadata":{"id":"FMlTvxAQqN0l"},"execution_count":null,"outputs":[]}]}